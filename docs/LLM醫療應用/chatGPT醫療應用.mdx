---
sidebar_position: 4
slug: chatGPT
title: chatGPT
authors:
  - name: John Lee
    title: Co-creator of Docusaurus 1
    url: https://github.com/JoelMarcey
    image_url: https://github.com/JoelMarcey.png
  - name: Sébastien Lorber
    title: Docusaurus maintainer
    url: https://sebastienlorber.com
    image_url: https://github.com/slorber.png
tags: [LLM, 大語言模型, chatGPT]
---

# chatGPT 的歷史以及由來

## chatGPT 原理

<iframe
  width="768"
  height="512"
  src="https://www.youtube.com/embed/sL1BNTU-4PI"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  allowfullscreen
></iframe>

https://www.youtube.com/watch?v=kCc8FmEb1nY&t=4s
ChatGPT 是由 OpenAI 開發的一個大型語言模型，它基於 GPT（Generative Pre-trained Transformer）的架構。GPT 是一個被設計用於處理自然語言處理任務的深度學習模型，它在大量的文本數據上進行預訓練，然後可以用於執行各種語言相關的任務，如文本生成、問答、翻譯等。
ChatGPT 的歷史可以追溯到 2015 年，當時 OpenAI 首次發布了一個基於循環神經網絡（Recurrent Neural Network）的模型，稱為 Seq2Seq。該模型的目標是進行序列到序列的學習，例如機器翻譯。然而，Seq2Seq 模型在處理長文本時遇到了一些問題，因為它無法捕捉到長期的依賴關係。
為了解決這個問題，Transformer 模型於 2017 年被提出，並且取得了巨大的成功。Transformer 模型使用了自注意力機制（Self-Attention），可以有效地處理長文本並捕捉到不同位置之間的關係。這對於語言模型來說是非常重要的，因為它需要理解上下文並生成合理的回答。
在 2018 年，OpenAI 推出了 GPT 模型，這是一個基於 Transformer 架構的語言模型。它在大量的互聯網文本數據上進行了預訓練，使得它能夠理解和生成自然語言。GPT 模型取得了巨大的成功，但它的大小和計算成本限制了它在實際應用中的使用。
為了繼續改進和發展語言模型，OpenAI 於 2020 年發布了 GPT-3，這是當時最大的語言模型，擁有 1.75 萬億個參數。GPT-3 在多個語言相關的任務上取得了驚人的成果，並展現出強大的語言理解和生成能力。
隨著 GPT-3 的成功，OpenAI 開始探索將這種語言模型應用於對話系統。他們進行了一系列的實驗和研究，ChatGPT 是在 GPT-3 的基礎上進一步改進和優化的，旨在實現更流暢、更具上下文理解能力的對話。OpenAI 通過在訓練數據中引入了對話文本，使模型能夠學習對話的特定模式和結構。
ChatGPT 的訓練過程與 GPT-3 類似，但輸入數據中包含了成對的問題和回答，這使得模型能夠學習生成有意義且具有上下文連貫性的回應。通過預訓練這樣的對話數據，ChatGPT 能夠學會回答問題、提供解釋、進行對話和執行其他相關任務。
然而，值得注意的是，ChatGPT 仍然有其限制和挑戰。它可能產生不正確或模棱兩可的答案，並且對於某些敏感或具有潛在風險的主題，可能需要進一步的人工審查和控制。
OpenAI 通過發布不同版本的 GPT 模型（如 GPT-2、GPT-3 和 ChatGPT），不斷推動語言模型的發展和應用。他們的目標是創建更加強大和可靠的語言模型，同時也要關注模型的倫理使用和風險管理。

## chatgpt 延伸應用

autoGPT
https://www.storm.mg/lifestyle/4775747?mode=whole
健身計畫
影養計畫
商品使用
